{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.0+cu116 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install pytorch-pretrained-bert\n",
    "!pip install livelossplot\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install unidecode\n",
    "!pip install pandas\n",
    "!pip install ir_datasets\n",
    "\n",
    "!pip install ipywidgets==7.* --user\n",
    "!pip install widgetsnbextension jupyter_contrib_nbextensions --user\n",
    "!jupyter contrib nbextension install --user\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "\n",
    "from util.bert import *\n",
    "from util.evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_filename = 'docs-dev.tsv'\n",
    "queries_filename = 'msmarco-docdev-queries.tsv'\n",
    "top100s_filename = 'msmarco-docdev-top100'\n",
    "qrels_filename = 'msmarco-docdev-qrels.tsv'\n",
    "\n",
    "query_ids_validation_filename = 'query-ids-dev-validation.tsv'\n",
    "query_ids_test_filename = 'query-ids-dev-test.tsv'\n",
    "\n",
    "model_dir = \"model/\"\n",
    "msmarco_dir = \"data/msmarco_doc/\"\n",
    "output_dir = \"data/output/\"\n",
    "\n",
    "top_n = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "folder = 'data/msmarco_doc/'\n",
    "file_names = [queries_filename, top100s_filename, qrels_filename]\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = folder + file_name\n",
    "    file_name_gz = file_name + '.gz'\n",
    "    file_path_gz = folder + file_name_gz\n",
    "    \n",
    "\n",
    "    if not exists(file_path_gz):\n",
    "        print('Downloading ' + file_name_gz + ' ...')\n",
    "        url = 'https://msmarco.blob.core.windows.net/msmarcoranking/' + file_name_gz\n",
    "        urllib.request.urlretrieve(url, file_path_gz)\n",
    "\n",
    "    if not exists(file_path):\n",
    "        print('Extracting ' + file_name_gz + ' ...')\n",
    "        with gzip.open(file_path_gz, 'rb') as file_gz:\n",
    "            with open(file_path, 'wb') as file:\n",
    "                shutil.copyfileobj(file_gz, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate document collection\n",
    "Must be executed in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"msmarco-document/trec-dl-2019\")\n",
    "doc_store = dataset.docs_store()\n",
    "\n",
    "# Load the validation and test query ids, and the top 100s\n",
    "query_ids_validation = pd.read_csv(\n",
    "    msmarco_dir + query_ids_validation_filename,\n",
    "    delimiter=' ', encoding='utf-8', header=None,\n",
    "    names=['query_id']\n",
    ")\n",
    "query_ids_test = pd.read_csv(\n",
    "    msmarco_dir + query_ids_test_filename,\n",
    "    delimiter=' ', encoding='utf-8', header=None,\n",
    "    names=['query_id']\n",
    ")\n",
    "top100s = pd.read_csv(\n",
    "    msmarco_dir + top100s_filename, \n",
    "    delimiter=' ', encoding='utf-8', header=None,\n",
    "    names = ['query_id', 'Q0', 'doc_id', 'initial_rank', 'score', 'run']\n",
    ")[['query_id', 'doc_id']]\n",
    "\n",
    "# Select the top 100s of the queries that are present in the test or validation dataset\n",
    "top100s_filtered = pd.concat([query_ids_validation, query_ids_test]).merge(top100s, how='left', on=['query_id'])\n",
    "\n",
    "# Create a new dataframe for the documents in the selected top 100s\n",
    "docs = pd.DataFrame(list(np.unique(top100s_filtered['doc_id'].tolist())), columns=['doc_id'])\n",
    "\n",
    "# Fetch the body of every document\n",
    "docs['body'] = docs['doc_id'].apply(lambda doc_id: doc_store.get(doc_id).body)\n",
    "\n",
    "# Save the document collection\n",
    "docs.to_csv(msmarco_dir + collection_filename, sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!python util/bert_convert_tensorflow_to_pytorch.py --tf_checkpoint_path=./model/BERT_Base_trained_on_MSMARCO/model.ckpt-100000 --bert_config_file=./model/BERT_Base_trained_on_MSMARCO/bert_config.json --pytorch_dump_path=./model/BERT_Base_trained_on_MSMARCO/pytorch.bin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_split_length(tokenizer, query, body):\n",
    "    return split_doc(query, body, tokenizer)\n",
    "\n",
    "def tokenize_split_period(tokenizer, query, body):\n",
    "    return split_doc(query, body, tokenizer, at_period=True)\n",
    "\n",
    "split_methods = {\n",
    "    'length': tokenize_split_length,\n",
    "    'period': tokenize_split_period,\n",
    "}\n",
    "\n",
    "n_chunks = 10\n",
    "\n",
    "# validation (True) or test (False) dataset\n",
    "validation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv(\n",
    "    msmarco_dir + collection_filename, \n",
    "    delimiter='\\t', encoding='utf-8', header=None,\n",
    "    names = ['doc_id', 'body']\n",
    ")\n",
    "\n",
    "queries = pd.read_csv(\n",
    "    msmarco_dir + queries_filename, \n",
    "    delimiter='\\t', encoding='utf-8', header=None, \n",
    "    names=['query_id', 'query']\n",
    ")\n",
    "\n",
    "top100s = pd.read_csv(\n",
    "    msmarco_dir + top100s_filename, \n",
    "    delimiter=' ', encoding='utf-8', header=None,\n",
    "    names=['query_id', 'Q0', 'doc_id', 'initial_rank', 'score', 'run']\n",
    ")[['query_id', 'doc_id', 'initial_rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "tqdm.write('General preprocessing...')\n",
    "\n",
    "# Query IDs\n",
    "data = pd.read_csv(msmarco_dir + (query_ids_validation_filename if validation else query_ids_test_filename), delimiter=' ', encoding='utf-8', header=None)\n",
    "data.columns = ['query_id']\n",
    "\n",
    "# Queries, top 100s\n",
    "data = data.merge(top100s[top100s['initial_rank'] <= top_n], how='left', on=['query_id'])\n",
    "data = data.merge(queries, how='left', on=['query_id'])\n",
    "data['query'] = data['query'].progress_apply(lambda x: remove_non_alphanumeric(x.lower()))\n",
    "\n",
    "# Docs\n",
    "data = data.merge(docs, how='left', on=['doc_id'])\n",
    "data['body'] = data['body'].progress_apply(lambda x: remove_non_alphanumeric(x.lower(), keep_periods=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', 2)\n",
    "model.load_state_dict(torch.load(model_dir + 'BERT_Base_trained_on_MSMARCO/pytorch.bin'))\n",
    "\n",
    "model.eval()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize + run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "logging.getLogger('pytorch_pretrained_bert').setLevel(logging.ERROR)\n",
    "\n",
    "query_ids = list(np.unique(data['query_id'].tolist()))\n",
    "query_id_chunks = list(split_chunks(query_ids, n_chunks))\n",
    "\n",
    "# Tokenization\n",
    "for method, tokenize in split_methods.items():\n",
    "    for i, query_id_chunk in enumerate(query_id_chunks):\n",
    "        file = output_dir + 'run.bert.{}.{}.tsv-{}-of-{}'.format(method, 'val' if validation else 'test', i + 1, n_chunks)\n",
    "        if exists(file):\n",
    "            continue\n",
    "\n",
    "        tqdm.write('Tokenization method \\'{}\\', chunk {}/{}'.format(method, i + 1, n_chunks))\n",
    "\n",
    "        data_chunk = data[data['query_id'].isin(query_id_chunk)].copy()\n",
    "\n",
    "        tqdm.write('Tokenizing ...')\n",
    "        data_chunk['input'] = data_chunk.progress_apply(lambda row: tokenize(tokenizer, row['query'], row['body']), axis=1)\n",
    "        data_chunk = data_chunk.explode('input')\n",
    "\n",
    "        tqdm.write('Running ...')\n",
    "        run_bert(model, data_chunk)\n",
    "\n",
    "        tqdm.write('Scoring ...')\n",
    "        data_chunk['score'] = data_chunk.progress_apply(lambda row: row['output'].data[0][1].item(), axis=1)\n",
    "\n",
    "        data_chunk[['query_id', 'doc_id', 'score']].to_csv(file,sep=\"\\t\", header=False,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame()\n",
    "\n",
    "for method in split_methods.keys():\n",
    "    filenames = [output_dir + 'run.bert.{}.{}.tsv-{}-of-{}'.format(method, 'val' if validation else 'test', i + 1, n_chunks) for i in range(n_chunks)]\n",
    "\n",
    "    dfs = []\n",
    "    for filename in filenames:\n",
    "        df = pd.read_csv(filename, delimiter='\\t', encoding='utf-8', header=None, names=['query_id', 'doc_id', 'score'])\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df.to_csv(output_dir + method + '.tsv', sep=' ', header=False, index=False)\n",
    "\n",
    "    for agg in ['sum', 'max', 'first']:\n",
    "        input_path = output_dir + '{}.tsv'.format(method)\n",
    "        output_path = output_dir + 'BERT-{}-{}-ranking.txt'.format(method, agg)\n",
    "        aggregate_results(input_path, output_path, agg, normalize=agg == 'sum')\n",
    "\n",
    "        results = evaluate(msmarco_dir + qrels_filename, output_path)\n",
    "        evaluation['metric'] = results.keys()\n",
    "        evaluation['{}-{}'.format(method, agg)] = results.values()\n",
    "\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('ir-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f918abf570f977e2e50fa3541a1cc05f77aa7d7291c6ebc3d2aa5f013fa00f4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
